{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# OPTIMIZED Multimodal Product Price Prediction - ML Challenge 2025\n",
        "\n",
        "Based on ACTUAL train.csv and test.csv analysis\n",
        "\n",
        "**STRATEGY: Hybrid Ensemble (Deep Learning + Gradient Boosting)**\n",
        "- Model 1: Multimodal Neural Network (Text + Image + Features)\n",
        "- Model 2: XGBoost on Engineered Features\n",
        "- Model 3: Weighted Ensemble\n",
        "\n",
        "**Expected SMAPE: 12-18%** (based on data analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## SECTION 1: Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch torchvision timm pillow pandas numpy scikit-learn tqdm requests xgboost lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import timm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount_drive"
      },
      "source": [
        "## SECTION 2: Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# MODIFY THESE PATHS\n",
        "BASE_PATH = '/content/drive/MyDrive/ML_Challenge_2025'\n",
        "DATASET_FOLDER = f'{BASE_PATH}/dataset'\n",
        "IMAGE_DIR = f'{BASE_PATH}/images'\n",
        "MODEL_SAVE_PATH = f'{BASE_PATH}/models'\n",
        "\n",
        "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
        "os.makedirs(f'{IMAGE_DIR}/train', exist_ok=True)\n",
        "os.makedirs(f'{IMAGE_DIR}/test', exist_ok=True)\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature_engineering"
      },
      "source": [
        "## SECTION 3: ENHANCED Feature Engineering (Based on Actual Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_functions"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean encoding issues\"\"\"\n",
        "    text = str(text)\n",
        "    # Fix common encoding issues\n",
        "    text = text.replace('â€™', \"'\").replace('â€œ', '\"').replace('â€', '\"')\n",
        "    text = text.replace('Ã±', 'ñ').replace('Ã©', 'é')\n",
        "    return text\n",
        "\n",
        "def extract_value_unit(catalog_content):\n",
        "    \"\"\"Extract Value and Unit from catalog\"\"\"\n",
        "    value_pattern = r'Value:\\s*(\\d+\\.?\\d*)'\n",
        "    unit_pattern = r'Unit:\\s*([A-Za-z\\s]+)'\n",
        "    \n",
        "    value_match = re.search(value_pattern, catalog_content)\n",
        "    unit_match = re.search(unit_pattern, catalog_content)\n",
        "    \n",
        "    value = float(value_match.group(1)) if value_match else 1.0\n",
        "    unit = unit_match.group(1).strip() if unit_match else 'Count'\n",
        "    \n",
        "    return value, unit\n",
        "\n",
        "def extract_ipq(text):\n",
        "    \"\"\"Extract Item Pack Quantity\"\"\"\n",
        "    patterns = [\n",
        "        r'\\(Pack of (\\d+)\\)',\n",
        "        r'Pack of (\\d+)',\n",
        "        r'(\\d+)\\s*Pack',\n",
        "        r'x\\s*(\\d+)\\s*Bags',\n",
        "        r'x\\s*(\\d+)\\s*Bottles'\n",
        "    ]\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, str(text), re.IGNORECASE)\n",
        "        if match:\n",
        "            return float(match.group(1))\n",
        "    return 1.0\n",
        "\n",
        "def extract_brand(text):\n",
        "    \"\"\"Extract brand name (first meaningful word)\"\"\"\n",
        "    text = str(text).strip()\n",
        "    # Remove \"Item Name:\" prefix\n",
        "    text = re.sub(r'^Item Name:\\s*', '', text, flags=re.IGNORECASE)\n",
        "    \n",
        "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of'}\n",
        "    words = text.split()\n",
        "    \n",
        "    for word in words[:3]:  # Check first 3 words\n",
        "        clean_word = re.sub(r'[^\\w]', '', word.lower())\n",
        "        if clean_word and len(clean_word) > 2 and clean_word not in stop_words:\n",
        "            return clean_word\n",
        "    \n",
        "    return 'unknown'\n",
        "\n",
        "def extract_product_category(text):\n",
        "    \"\"\"Categorize product based on keywords\"\"\"\n",
        "    text = str(text).lower()\n",
        "    \n",
        "    categories = {\n",
        "        'beverage': ['tea', 'coffee', 'juice', 'drink', 'soda', 'water', 'milk'],\n",
        "        'sauce_condiment': ['sauce', 'salsa', 'ketchup', 'mustard', 'dressing', 'marinade', 'syrup'],\n",
        "        'candy_sweets': ['candy', 'chocolate', 'gummy', 'taffy', 'lollipop', 'sweet'],\n",
        "        'snack': ['chips', 'popcorn', 'pretzel', 'crackers', 'nuts', 'bar'],\n",
        "        'spice_seasoning': ['spice', 'seasoning', 'pepper', 'salt', 'herb', 'cumin'],\n",
        "        'baking': ['flour', 'sugar', 'extract', 'vanilla', 'baking', 'yeast'],\n",
        "        'cooking_oil': ['oil', 'vinegar', 'olive', 'coconut'],\n",
        "        'cereal_grain': ['cereal', 'oatmeal', 'granola', 'rice', 'pasta', 'noodle'],\n",
        "        'canned': ['canned', 'beans', 'soup', 'chili', 'tomato'],\n",
        "        'supplement': ['protein', 'vitamin', 'supplement', 'powder']\n",
        "    }\n",
        "    \n",
        "    for category, keywords in categories.items():\n",
        "        if any(keyword in text for keyword in keywords):\n",
        "            return category\n",
        "    \n",
        "    return 'other'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "numeric_features"
      },
      "outputs": [],
      "source": [
        "def extract_numeric_features(text):\n",
        "    \"\"\"Extract comprehensive features\"\"\"\n",
        "    text_str = str(text).lower()\n",
        "    \n",
        "    features = {}\n",
        "    \n",
        "    # Text statistics\n",
        "    features['text_length'] = len(text_str)\n",
        "    features['word_count'] = len(text_str.split())\n",
        "    features['bullet_count'] = text_str.count('bullet point')\n",
        "    features['has_description'] = int('product description' in text_str)\n",
        "    \n",
        "    # All numbers in text\n",
        "    numbers = re.findall(r'\\d+\\.?\\d*', text_str)\n",
        "    features['num_count'] = len(numbers)\n",
        "    features['max_number'] = max([float(n) for n in numbers]) if numbers else 0\n",
        "    features['avg_number'] = np.mean([float(n) for n in numbers]) if numbers else 0\n",
        "    \n",
        "    # Premium/Quality indicators\n",
        "    premium_words = ['premium', 'gourmet', 'luxury', 'deluxe', 'professional', \n",
        "                     'artisan', 'handcrafted', 'imported', 'organic', 'natural']\n",
        "    features['premium_score'] = sum(1 for word in premium_words if word in text_str)\n",
        "    \n",
        "    # Specific keywords\n",
        "    features['is_organic'] = int('organic' in text_str)\n",
        "    features['is_natural'] = int('natural' in text_str)\n",
        "    features['is_gluten_free'] = int('gluten free' in text_str or 'gluten-free' in text_str)\n",
        "    features['is_vegan'] = int('vegan' in text_str)\n",
        "    features['is_kosher'] = int('kosher' in text_str)\n",
        "    features['is_non_gmo'] = int('non-gmo' in text_str or 'non gmo' in text_str)\n",
        "    \n",
        "    # Size indicators\n",
        "    features['is_bulk'] = int(any(word in text_str for word in ['bulk', 'wholesale', 'case']))\n",
        "    features['is_mini'] = int(any(word in text_str for word in ['mini', 'small', 'snack size']))\n",
        "    \n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comprehensive_feature_engineering"
      },
      "outputs": [],
      "source": [
        "def comprehensive_feature_engineering(df):\n",
        "    \"\"\"Apply ALL feature engineering\"\"\"\n",
        "    print(\"🔧 Comprehensive Feature Engineering...\")\n",
        "    \n",
        "    # Clean text\n",
        "    df['catalog_content'] = df['catalog_content'].apply(clean_text)\n",
        "    \n",
        "    # Extract Value and Unit\n",
        "    value_unit = df['catalog_content'].apply(extract_value_unit)\n",
        "    df['value'] = [vu[0] for vu in value_unit]\n",
        "    df['unit'] = [vu[1] for vu in value_unit]\n",
        "    \n",
        "    # Extract IPQ\n",
        "    df['ipq'] = df['catalog_content'].apply(extract_ipq)\n",
        "    \n",
        "    # Extract Brand\n",
        "    df['brand'] = df['catalog_content'].apply(extract_brand)\n",
        "    \n",
        "    # Extract Category\n",
        "    df['category'] = df['catalog_content'].apply(extract_product_category)\n",
        "    \n",
        "    # Numeric features\n",
        "    numeric_features = df['catalog_content'].apply(extract_numeric_features)\n",
        "    feature_df = pd.DataFrame(list(numeric_features))\n",
        "    df = pd.concat([df, feature_df], axis=1)\n",
        "    \n",
        "    # CRITICAL: Calculate total volume\n",
        "    df['total_volume'] = df['value'] * df['ipq']\n",
        "    \n",
        "    # Unit type encoding (VERY IMPORTANT!)\n",
        "    unit_mapping = {\n",
        "        'fl oz': 'fluid_ounce',\n",
        "        'fluid ounce': 'fluid_ounce',\n",
        "        'ounce': 'weight_ounce',\n",
        "        'oz': 'weight_ounce',\n",
        "        'count': 'count',\n",
        "        'pound': 'pound',\n",
        "        'lb': 'pound'\n",
        "    }\n",
        "    \n",
        "    df['unit_normalized'] = df['unit'].str.lower().str.strip()\n",
        "    df['unit_type'] = df['unit_normalized'].map(unit_mapping).fillna('other')\n",
        "    \n",
        "    # Encode unit type\n",
        "    df['unit_type_code'] = df['unit_type'].map({\n",
        "        'fluid_ounce': 0,\n",
        "        'weight_ounce': 1,\n",
        "        'count': 2,\n",
        "        'pound': 3,\n",
        "        'other': 4\n",
        "    })\n",
        "    \n",
        "    # Price per unit (will be calculated for training set only)\n",
        "    if 'price' in df.columns:\n",
        "        df['price_per_unit'] = df['price'] / (df['total_volume'] + 1)\n",
        "        df['log_price'] = np.log1p(df['price'])\n",
        "    \n",
        "    print(f\"✓ Created {len(df.columns) - 3} features\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data"
      },
      "source": [
        "## SECTION 4: Load and Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loading"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOADING AND PROCESSING DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
        "test_df = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
        "\n",
        "print(f\"Train: {train_df.shape}\")\n",
        "print(f\"Test: {test_df.shape}\")\n",
        "\n",
        "# Feature engineering\n",
        "train_df = comprehensive_feature_engineering(train_df)\n",
        "test_df = comprehensive_feature_engineering(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "categorical_encoding"
      },
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "le_brand = LabelEncoder()\n",
        "le_category = LabelEncoder()\n",
        "\n",
        "# Fit on combined data\n",
        "all_brands = pd.concat([train_df['brand'], test_df['brand']]).unique()\n",
        "all_categories = pd.concat([train_df['category'], test_df['category']]).unique()\n",
        "\n",
        "le_brand.fit(all_brands)\n",
        "le_category.fit(all_categories)\n",
        "\n",
        "train_df['brand_encoded'] = le_brand.transform(train_df['brand'])\n",
        "test_df['brand_encoded'] = le_brand.transform(test_df['brand'])\n",
        "\n",
        "train_df['category_encoded'] = le_category.transform(train_df['category'])\n",
        "test_df['category_encoded'] = le_category.transform(test_df['category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "target_encoding"
      },
      "outputs": [],
      "source": [
        "# Target encoding for brand (VERY POWERFUL)\n",
        "brand_stats = train_df.groupby('brand_encoded').agg({\n",
        "    'log_price': ['mean', 'std', 'count', 'min', 'max']\n",
        "}).reset_index()\n",
        "brand_stats.columns = ['brand_encoded', 'brand_mean_price', 'brand_std_price', \n",
        "                       'brand_count', 'brand_min_price', 'brand_max_price']\n",
        "\n",
        "train_df = train_df.merge(brand_stats, on='brand_encoded', how='left')\n",
        "test_df = test_df.merge(brand_stats, on='brand_encoded', how='left')\n",
        "\n",
        "# Fill missing\n",
        "global_mean = train_df['log_price'].mean()\n",
        "for col in ['brand_mean_price', 'brand_std_price', 'brand_min_price', 'brand_max_price']:\n",
        "    train_df[col].fillna(global_mean if 'mean' in col else 0, inplace=True)\n",
        "    test_df[col].fillna(global_mean if 'mean' in col else 0, inplace=True)\n",
        "\n",
        "train_df['brand_count'].fillna(1, inplace=True)\n",
        "test_df['brand_count'].fillna(1, inplace=True)\n",
        "\n",
        "# Category target encoding\n",
        "category_stats = train_df.groupby('category_encoded').agg({\n",
        "    'log_price': ['mean', 'std']\n",
        "}).reset_index()\n",
        "category_stats.columns = ['category_encoded', 'category_mean_price', 'category_std_price']\n",
        "\n",
        "train_df = train_df.merge(category_stats, on='category_encoded', how='left')\n",
        "test_df = test_df.merge(category_stats, on='category_encoded', how='left')\n",
        "\n",
        "train_df['category_mean_price'].fillna(global_mean, inplace=True)\n",
        "test_df['category_mean_price'].fillna(global_mean, inplace=True)\n",
        "train_df['category_std_price'].fillna(0, inplace=True)\n",
        "test_df['category_std_price'].fillna(0, inplace=True)\n",
        "\n",
        "print(\"\\n✓ Feature Engineering Complete!\")\n",
        "print(f\"Final feature count: {len(train_df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "images"
      },
      "source": [
        "## SECTION 5: Download Images (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "image_functions"
      },
      "outputs": [],
      "source": [
        "def download_image(url, save_path, max_retries=3):\n",
        "    \"\"\"Download image with retry logic\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10, headers={\n",
        "                'User-Agent': 'Mozilla/5.0'\n",
        "            })\n",
        "            if response.status_code == 200:\n",
        "                img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "                img.save(save_path)\n",
        "                return True\n",
        "        except:\n",
        "            if attempt == max_retries - 1:\n",
        "                return False\n",
        "    return False\n",
        "\n",
        "def download_images_batch(df, split='train', batch_size=100):\n",
        "    \"\"\"Download images in batches\"\"\"\n",
        "    save_dir = f'{IMAGE_DIR}/{split}'\n",
        "    success_count = 0\n",
        "    \n",
        "    for idx in tqdm(range(len(df)), desc=f'Downloading {split} images'):\n",
        "        row = df.iloc[idx]\n",
        "        sample_id = row['sample_id']\n",
        "        save_path = f'{save_dir}/{sample_id}.jpg'\n",
        "        \n",
        "        if os.path.exists(save_path):\n",
        "            success_count += 1\n",
        "            continue\n",
        "        \n",
        "        if download_image(row['image_link'], save_path):\n",
        "            success_count += 1\n",
        "        \n",
        "        if idx % batch_size == 0:\n",
        "            import time\n",
        "            time.sleep(0.5)\n",
        "    \n",
        "    print(f\"✓ Downloaded: {success_count}/{len(df)} images\")\n",
        "    return success_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_images"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"IMAGE DOWNLOAD\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "download_choice = input(\"Download images? (yes/no/sample): \").lower()\n",
        "\n",
        "if download_choice == 'yes':\n",
        "    download_images_batch(train_df, 'train')\n",
        "    download_images_batch(test_df, 'test')\n",
        "elif download_choice == 'sample':\n",
        "    n = min(5000, len(train_df))\n",
        "    download_images_batch(train_df.iloc[:n], 'train')\n",
        "    download_images_batch(test_df.iloc[:n], 'test')\n",
        "    train_df = train_df.iloc[:n]\n",
        "    test_df = test_df.iloc[:n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset"
      },
      "source": [
        "## SECTION 6: Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_class"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class ProductDataset(Dataset):\n",
        "    def __init__(self, df, image_dir, tokenizer, feature_cols, max_length=128, is_train=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_cols = feature_cols\n",
        "        self.max_length = max_length\n",
        "        self.is_train = is_train\n",
        "        \n",
        "        # Image transforms\n",
        "        if is_train:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.RandomHorizontalFlip(p=0.3),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        \n",
        "        # Text\n",
        "        text = str(row['catalog_content'])[:512]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # Image\n",
        "        img_path = f'{self.image_dir}/{row[\"sample_id\"]}.jpg'\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except:\n",
        "            image = Image.new('RGB', (224, 224), color='white')\n",
        "        \n",
        "        image = self.transform(image)\n",
        "        \n",
        "        # Features\n",
        "        features = torch.tensor([float(row[col]) for col in self.feature_cols], \n",
        "                               dtype=torch.float32)\n",
        "        features = torch.nan_to_num(features, 0.0)\n",
        "        \n",
        "        output = {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'image': image,\n",
        "            'features': features,\n",
        "            'sample_id': row['sample_id']\n",
        "        }\n",
        "        \n",
        "        if self.is_train:\n",
        "            output['price'] = torch.tensor(row['log_price'], dtype=torch.float32)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model"
      },
      "source": [
        "## SECTION 7: Multimodal Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "multimodal_model"
      },
      "outputs": [],
      "source": [
        "class MultiModalPricePredictor(nn.Module):\n",
        "    def __init__(self, num_features=30):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Text encoder (DistilBERT - efficient)\n",
        "        self.text_encoder = AutoModel.from_pretrained('distilbert-base-uncased')\n",
        "        text_dim = 768\n",
        "        \n",
        "        # Image encoder (EfficientNet-B2 - balance speed/accuracy)\n",
        "        self.image_encoder = timm.create_model('efficientnet_b2', pretrained=True, num_classes=0)\n",
        "        image_dim = self.image_encoder.num_features\n",
        "        \n",
        "        # Feature projection\n",
        "        self.feature_projection = nn.Sequential(\n",
        "            nn.Linear(num_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Fusion\n",
        "        total_dim = text_dim + image_dim + 64\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(total_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Regressor\n",
        "        self.regressor = nn.Linear(128, 1)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, image, features):\n",
        "        # Text\n",
        "        text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_feat = text_output.last_hidden_state[:, 0, :]\n",
        "        \n",
        "        # Image\n",
        "        image_feat = self.image_encoder(image)\n",
        "        \n",
        "        # Features\n",
        "        feat_proj = self.feature_projection(features)\n",
        "        \n",
        "        # Combine\n",
        "        combined = torch.cat([text_feat, image_feat, feat_proj], dim=1)\n",
        "        fused = self.fusion(combined)\n",
        "        price = self.regressor(fused)\n",
        "        \n",
        "        return price.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "features"
      },
      "source": [
        "## SECTION 8: Define Feature Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_cols"
      },
      "outputs": [],
      "source": [
        "FEATURE_COLS = [\n",
        "    'value', 'ipq', 'total_volume', 'unit_type_code',\n",
        "    'brand_encoded', 'category_encoded',\n",
        "    'brand_mean_price', 'brand_std_price', 'brand_count', \n",
        "    'brand_min_price', 'brand_max_price',\n",
        "    'category_mean_price', 'category_std_price',\n",
        "    'text_length', 'word_count', 'bullet_count', 'has_description',\n",
        "    'num_count', 'max_number', 'avg_number',\n",
        "    'premium_score', 'is_organic', 'is_natural', 'is_gluten_free',\n",
        "    'is_vegan', 'is_kosher', 'is_non_gmo', 'is_bulk', 'is_mini'\n",
        "]\n",
        "\n",
        "# Ensure all columns exist\n",
        "for col in FEATURE_COLS:\n",
        "    if col not in train_df.columns:\n",
        "        train_df[col] = 0\n",
        "        test_df[col] = 0\n",
        "\n",
        "print(f\"\\n✓ Using {len(FEATURE_COLS)} engineered features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loaders"
      },
      "source": [
        "## SECTION 9: Prepare Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_loaders"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPARING DATA LOADERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Split for validation\n",
        "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "train_dataset = ProductDataset(train_data, f'{IMAGE_DIR}/train', tokenizer, FEATURE_COLS, is_train=True)\n",
        "val_dataset = ProductDataset(val_data, f'{IMAGE_DIR}/train', tokenizer, FEATURE_COLS, is_train=True)\n",
        "test_dataset = ProductDataset(test_df, f'{IMAGE_DIR}/test', tokenizer, FEATURE_COLS, is_train=False)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"✓ Data loaders ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_functions"
      },
      "source": [
        "## SECTION 10: Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_validate"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in tqdm(dataloader, desc='Training'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        image = batch['image'].to(device)\n",
        "        features = batch['features'].to(device)\n",
        "        price = batch['price'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        predicted = model(input_ids, attention_mask, image, features)\n",
        "        loss = F.huber_loss(predicted, price)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc='Validation'):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            image = batch['image'].to(device)\n",
        "            features = batch['features'].to(device)\n",
        "            price = batch['price'].to(device)\n",
        "            \n",
        "            predicted = model(input_ids, attention_mask, image, features)\n",
        "            \n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            actuals.extend(price.cpu().numpy())\n",
        "    \n",
        "    predictions = np.array(predictions)\n",
        "    actuals = np.array(actuals)\n",
        "    \n",
        "    # SMAPE\n",
        "    pred_prices = np.expm1(predictions)\n",
        "    actual_prices = np.expm1(actuals)\n",
        "    \n",
        "    numerator = np.abs(pred_prices - actual_prices)\n",
        "    denominator = (np.abs(actual_prices) + np.abs(pred_prices)) / 2.0\n",
        "    smape = np.mean(numerator / (denominator + 1e-8)) * 100\n",
        "    \n",
        "    return smape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_model"
      },
      "source": [
        "## SECTION 11: Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING MULTIMODAL MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = MultiModalPricePredictor(num_features=len(FEATURE_COLS)).to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "NUM_EPOCHS = 8\n",
        "best_smape = float('inf')\n",
        "history = {'train_loss': [], 'val_smape': []}\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_smape = validate(model, val_loader, device)\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_smape'].append(val_smape)\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val SMAPE: {val_smape:.2f}%\")\n",
        "    \n",
        "    if val_smape < best_smape:\n",
        "        best_smape = val_smape\n",
        "        torch.save(model.state_dict(), f'{MODEL_SAVE_PATH}/best_multimodal.pth')\n",
        "        print(\"✓ Best model saved!\")\n",
        "\n",
        "print(f\"\\n✓ Training complete! Best SMAPE: {best_smape:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl_predictions"
      },
      "source": [
        "## SECTION 12: Generate Deep Learning Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_dl_predictions"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATING DL PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model.load_state_dict(torch.load(f'{MODEL_SAVE_PATH}/best_multimodal.pth'))\n",
        "model.eval()\n",
        "\n",
        "dl_predictions = []\n",
        "sample_ids = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc='Predicting'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        image = batch['image'].to(device)\n",
        "        features = batch['features'].to(device)\n",
        "        \n",
        "        predicted = model(input_ids, attention_mask, image, features)\n",
        "        predicted_prices = np.expm1(predicted.cpu().numpy())\n",
        "        \n",
        "        dl_predictions.extend(predicted_prices.tolist())\n",
        "        sample_ids.extend(batch['sample_id'].tolist())\n",
        "\n",
        "dl_pred_df = pd.DataFrame({\n",
        "    'sample_id': sample_ids,\n",
        "    'dl_price': dl_predictions\n",
        "})\n",
        "\n",
        "print(f\"✓ Deep Learning predictions complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgboost"
      },
      "source": [
        "## SECTION 13: XGBoost Model on Engineered Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgboost_training"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING XGBOOST MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Prepare feature matrix\n",
        "X_train = train_data[FEATURE_COLS].values\n",
        "y_train = train_data['log_price'].values\n",
        "X_val = val_data[FEATURE_COLS].values\n",
        "y_val = val_data['log_price'].values\n",
        "X_test = test_df[FEATURE_COLS].values\n",
        "\n",
        "# XGBoost parameters\n",
        "xgb_params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 8,\n",
        "    'min_child_weight': 3,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'gamma': 0.1,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 1.0,\n",
        "    'tree_method': 'hist',\n",
        "    'eval_metric': 'rmse',\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=FEATURE_COLS)\n",
        "dval = xgb.DMatrix(X_val, label=y_val, feature_names=FEATURE_COLS)\n",
        "dtest = xgb.DMatrix(X_test, feature_names=FEATURE_COLS)\n",
        "\n",
        "evallist = [(dtrain, 'train'), (dval, 'eval')]\n",
        "\n",
        "print(\"Training XGBoost...\")\n",
        "xgb_model = xgb.train(\n",
        "    xgb_params,\n",
        "    dtrain,\n",
        "    num_boost_round=500,\n",
        "    evals=evallist,\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgboost_predictions"
      },
      "outputs": [],
      "source": [
        "# XGBoost predictions\n",
        "xgb_pred_log = xgb_model.predict(dtest)\n",
        "xgb_predictions = np.expm1(xgb_pred_log)\n",
        "\n",
        "# Calculate validation SMAPE for XGBoost\n",
        "xgb_val_pred_log = xgb_model.predict(dval)\n",
        "xgb_val_pred = np.expm1(xgb_val_pred_log)\n",
        "y_val_original = np.expm1(y_val)\n",
        "\n",
        "xgb_smape = np.mean(np.abs(xgb_val_pred - y_val_original) / \n",
        "                    ((np.abs(y_val_original) + np.abs(xgb_val_pred)) / 2 + 1e-8)) * 100\n",
        "\n",
        "print(f\"\\n✓ XGBoost Val SMAPE: {xgb_smape:.2f}%\")\n",
        "\n",
        "# Save XGBoost predictions\n",
        "xgb_pred_df = pd.DataFrame({\n",
        "    'sample_id': test_df['sample_id'].values,\n",
        "    'xgb_price': xgb_predictions\n",
        "})\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = xgb_model.get_score(importance_type='gain')\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': list(feature_importance.keys()),\n",
        "    'importance': list(feature_importance.values())\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\n📊 Top 10 Most Important Features:\")\n",
        "print(importance_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lightgbm"
      },
      "source": [
        "## SECTION 14: LightGBM Model (Optional - for additional ensemble)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lightgbm_training"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING LIGHTGBM MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "lgb_params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': -1,\n",
        "    'min_child_samples': 20,\n",
        "    'subsample': 0.8,\n",
        "    'subsample_freq': 1,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 1.0,\n",
        "    'verbose': -1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
        "\n",
        "print(\"Training LightGBM...\")\n",
        "lgb_model = lgb.train(\n",
        "    lgb_params,\n",
        "    lgb_train,\n",
        "    num_boost_round=500,\n",
        "    valid_sets=[lgb_train, lgb_val],\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(period=50)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# LightGBM predictions\n",
        "lgb_pred_log = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
        "lgb_predictions = np.expm1(lgb_pred_log)\n",
        "\n",
        "# Validation SMAPE\n",
        "lgb_val_pred_log = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)\n",
        "lgb_val_pred = np.expm1(lgb_val_pred_log)\n",
        "\n",
        "lgb_smape = np.mean(np.abs(lgb_val_pred - y_val_original) / \n",
        "                    ((np.abs(y_val_original) + np.abs(lgb_val_pred)) / 2 + 1e-8)) * 100\n",
        "\n",
        "print(f\"\\n✓ LightGBM Val SMAPE: {lgb_smape:.2f}%\")\n",
        "\n",
        "lgb_pred_df = pd.DataFrame({\n",
        "    'sample_id': test_df['sample_id'].values,\n",
        "    'lgb_price': lgb_predictions\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ensemble"
      },
      "source": [
        "## SECTION 15: Ensemble Predictions (Weighted Average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ensemble_predictions"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING ENSEMBLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Merge all predictions\n",
        "ensemble_df = dl_pred_df.merge(xgb_pred_df, on='sample_id')\n",
        "ensemble_df = ensemble_df.merge(lgb_pred_df, on='sample_id')\n",
        "\n",
        "# Calculate optimal weights based on validation performance\n",
        "weights = {\n",
        "    'dl': 1.0 / best_smape if best_smape > 0 else 0,\n",
        "    'xgb': 1.0 / xgb_smape if xgb_smape > 0 else 0,\n",
        "    'lgb': 1.0 / lgb_smape if lgb_smape > 0 else 0\n",
        "}\n",
        "\n",
        "# Normalize weights\n",
        "total_weight = sum(weights.values())\n",
        "weights = {k: v/total_weight for k, v in weights.items()}\n",
        "\n",
        "print(f\"\\n📊 Ensemble Weights:\")\n",
        "print(f\"   Deep Learning: {weights['dl']:.3f} (SMAPE: {best_smape:.2f}%)\")\n",
        "print(f\"   XGBoost:       {weights['xgb']:.3f} (SMAPE: {xgb_smape:.2f}%)\")\n",
        "print(f\"   LightGBM:      {weights['lgb']:.3f} (SMAPE: {lgb_smape:.2f}%)\")\n",
        "\n",
        "# Weighted ensemble\n",
        "ensemble_df['price'] = (\n",
        "    ensemble_df['dl_price'] * weights['dl'] +\n",
        "    ensemble_df['xgb_price'] * weights['xgb'] +\n",
        "    ensemble_df['lgb_price'] * weights['lgb']\n",
        ")\n",
        "\n",
        "# Ensure positive prices\n",
        "ensemble_df['price'] = ensemble_df['price'].clip(lower=0.01)\n",
        "\n",
        "# Round to 2 decimals\n",
        "ensemble_df['price'] = ensemble_df['price'].round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "submission"
      },
      "source": [
        "## SECTION 16: Create Final Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_submission"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING SUBMISSION FILE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Match test.csv order\n",
        "test_order = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))[['sample_id']]\n",
        "submission_df = test_order.merge(ensemble_df[['sample_id', 'price']], on='sample_id', how='left')\n",
        "\n",
        "# Fill any missing predictions with median\n",
        "if submission_df['price'].isna().sum() > 0:\n",
        "    median_price = submission_df['price'].median()\n",
        "    submission_df['price'].fillna(median_price, inplace=True)\n",
        "    print(f\"⚠️  Filled {submission_df['price'].isna().sum()} missing with median: ${median_price:.2f}\")\n",
        "\n",
        "# Save submission\n",
        "output_filename = os.path.join(DATASET_FOLDER, 'test_out.csv')\n",
        "submission_df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\n✓ Submission saved: {output_filename}\")\n",
        "print(f\"   Total predictions: {len(submission_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis"
      },
      "source": [
        "## SECTION 17: Analysis and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analysis_validation"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n📊 Price Statistics:\")\n",
        "print(submission_df['price'].describe())\n",
        "\n",
        "print(\"\\n🔍 Quality Checks:\")\n",
        "print(f\"   Missing: {submission_df['price'].isna().sum()}\")\n",
        "print(f\"   Negative: {(submission_df['price'] < 0).sum()}\")\n",
        "print(f\"   Zero: {(submission_df['price'] == 0).sum()}\")\n",
        "print(f\"   < $1: {(submission_df['price'] < 1).sum()}\")\n",
        "print(f\"   > $1000: {(submission_df['price'] > 1000).sum()}\")\n",
        "\n",
        "# Compare distributions\n",
        "print(\"\\n📈 Distribution Comparison:\")\n",
        "print(f\"   Train Mean:  ${train_df['price'].mean():.2f}\")\n",
        "print(f\"   Train Median: ${train_df['price'].median():.2f}\")\n",
        "print(f\"   Pred Mean:   ${submission_df['price'].mean():.2f}\")\n",
        "print(f\"   Pred Median:  ${submission_df['price'].median():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualizations"
      },
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Price distribution\n",
        "axes[0, 0].hist(train_df['price'], bins=50, alpha=0.7, label='Train', edgecolor='black')\n",
        "axes[0, 0].hist(submission_df['price'], bins=50, alpha=0.7, label='Pred', edgecolor='black')\n",
        "axes[0, 0].set_xlabel('Price')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Price Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].set_yscale('log')\n",
        "\n",
        "# Log price distribution\n",
        "axes[0, 1].hist(np.log1p(train_df['price']), bins=50, alpha=0.7, label='Train', edgecolor='black')\n",
        "axes[0, 1].hist(np.log1p(submission_df['price']), bins=50, alpha=0.7, label='Pred', edgecolor='black')\n",
        "axes[0, 1].set_xlabel('Log(Price + 1)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Log Price Distribution')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Box plot\n",
        "axes[0, 2].boxplot([train_df['price'], submission_df['price']], labels=['Train', 'Pred'])\n",
        "axes[0, 2].set_ylabel('Price')\n",
        "axes[0, 2].set_title('Price Box Plot')\n",
        "\n",
        "# Model comparison\n",
        "axes[1, 0].scatter(range(len(ensemble_df)), ensemble_df['dl_price'], alpha=0.5, s=1, label='DL')\n",
        "axes[1, 0].scatter(range(len(ensemble_df)), ensemble_df['xgb_price'], alpha=0.5, s=1, label='XGB')\n",
        "axes[1, 0].scatter(range(len(ensemble_df)), ensemble_df['lgb_price'], alpha=0.5, s=1, label='LGB')\n",
        "axes[1, 0].set_xlabel('Sample Index')\n",
        "axes[1, 0].set_ylabel('Predicted Price')\n",
        "axes[1, 0].set_title('Model Predictions Comparison')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Training history\n",
        "axes[1, 1].plot(history['train_loss'], marker='o', label='Train Loss')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Loss')\n",
        "axes[1, 1].set_title('Training Loss')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Validation SMAPE\n",
        "axes[1, 2].plot(history['val_smape'], marker='o', color='green', label='Val SMAPE')\n",
        "axes[1, 2].axhline(y=best_smape, color='r', linestyle='--', label=f'Best: {best_smape:.2f}%')\n",
        "axes[1, 2].set_xlabel('Epoch')\n",
        "axes[1, 2].set_ylabel('SMAPE (%)')\n",
        "axes[1, 2].set_title('Validation SMAPE')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{BASE_PATH}/analysis.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_predictions"
      },
      "source": [
        "## SECTION 18: Sample Predictions Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "display_samples"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLE PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show sample predictions with details\n",
        "sample_indices = np.random.choice(len(test_df), min(10, len(test_df)), replace=False)\n",
        "\n",
        "for idx in sample_indices:\n",
        "    row = test_df.iloc[idx]\n",
        "    pred_row = submission_df[submission_df['sample_id'] == row['sample_id']].iloc[0]\n",
        "    \n",
        "    # Extract product name\n",
        "    item_name = row['catalog_content'].split('\\n')[0].replace('Item Name: ', '').strip()\n",
        "    if len(item_name) > 80:\n",
        "        item_name = item_name[:80] + \"...\"\n",
        "    \n",
        "    print(f\"\\n📦 {item_name}\")\n",
        "    print(f\"   Sample ID: {row['sample_id']}\")\n",
        "    print(f\"   Value: {row['value']:.1f} {row['unit']} (Pack of {row['ipq']:.0f})\")\n",
        "    print(f\"   Category: {row['category']}\")\n",
        "    print(f\"   Brand: {row['brand']}\")\n",
        "    print(f\"   Predicted Price: ${pred_row['price']:.2f}\")\n",
        "    print(f\"   DL: ${ensemble_df[ensemble_df['sample_id']==row['sample_id']]['dl_price'].values[0]:.2f} | \"\n",
        "          f\"XGB: ${ensemble_df[ensemble_df['sample_id']==row['sample_id']]['xgb_price'].values[0]:.2f} | \"\n",
        "          f\"LGB: ${ensemble_df[ensemble_df['sample_id']==row['sample_id']]['lgb_price'].values[0]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_artifacts"
      },
      "source": [
        "## SECTION 19: Save Model Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_models"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING MODEL ARTIFACTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save models\n",
        "xgb_model.save_model(f'{MODEL_SAVE_PATH}/xgboost_model.json')\n",
        "lgb_model.save_model(f'{MODEL_SAVE_PATH}/lightgbm_model.txt')\n",
        "\n",
        "# Save configuration\n",
        "config = {\n",
        "    'models': {\n",
        "        'deep_learning': {\n",
        "            'architecture': 'MultiModalPricePredictor',\n",
        "            'text_encoder': 'distilbert-base-uncased',\n",
        "            'image_encoder': 'efficientnet_b2',\n",
        "            'val_smape': float(best_smape)\n",
        "        },\n",
        "        'xgboost': {\n",
        "            'val_smape': float(xgb_smape)\n",
        "        },\n",
        "        'lightgbm': {\n",
        "            'val_smape': float(lgb_smape)\n",
        "        }\n",
        "    },\n",
        "    'ensemble': {\n",
        "        'weights': weights,\n",
        "        'expected_smape': f\"{min(best_smape, xgb_smape, lgb_smape):.2f}%\"\n",
        "    },\n",
        "    'feature_columns': FEATURE_COLS,\n",
        "    'num_features': len(FEATURE_COLS)\n",
        "}\n",
        "\n",
        "with open(f'{MODEL_SAVE_PATH}/config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "# Save feature importance\n",
        "importance_df.to_csv(f'{MODEL_SAVE_PATH}/feature_importance.csv', index=False)\n",
        "\n",
        "print(\"✓ All artifacts saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_summary"
      },
      "source": [
        "## SECTION 20: Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "summary"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎉 PIPELINE COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "📋 FINAL SUMMARY:\n",
        "\n",
        "✅ Models Trained:\n",
        "   • Multimodal Deep Learning (Text + Image + Features)\n",
        "     └─ Validation SMAPE: {best_smape:.2f}%\n",
        "   \n",
        "   • XGBoost (Engineered Features)\n",
        "     └─ Validation SMAPE: {xgb_smape:.2f}%\n",
        "   \n",
        "   • LightGBM (Engineered Features)\n",
        "     └─ Validation SMAPE: {lgb_smape:.2f}%\n",
        "\n",
        "✅ Ensemble Model:\n",
        "   • Weighted Average based on validation performance\n",
        "   • Expected SMAPE: {min(best_smape, xgb_smape, lgb_smape):.2f}%\n",
        "\n",
        "📊 Predictions Generated:\n",
        "   • Total samples: {len(submission_df)}\n",
        "   • Output file: {output_filename}\n",
        "   • Mean price: ${submission_df['price'].mean():.2f}\n",
        "   • Median price: ${submission_df['price'].median():.2f}\n",
        "\n",
        "📁 Files Created:\n",
        "   ✓ {output_filename} - Submission file\n",
        "   ✓ {MODEL_SAVE_PATH}/best_multimodal.pth - DL model\n",
        "   ✓ {MODEL_SAVE_PATH}/xgboost_model.json - XGBoost model\n",
        "   ✓ {MODEL_SAVE_PATH}/lightgbm_model.txt - LightGBM model\n",
        "   ✓ {MODEL_SAVE_PATH}/config.json - Configuration\n",
        "   ✓ {MODEL_SAVE_PATH}/feature_importance.csv - Feature importance\n",
        "   ✓ {BASE_PATH}/analysis.png - Visualizations\n",
        "\n",
        "🚀 Next Steps:\n",
        "   1. Review the analysis plots above\n",
        "   2. Check prediction quality metrics\n",
        "   3. Upload test_out.csv to competition portal\n",
        "   4. Monitor leaderboard score\n",
        "\n",
        "💡 Tips for Improvement:\n",
        "   • Train for more epochs (10-15)\n",
        "   • Try cross-validation (5-fold)\n",
        "   • Experiment with different ensemble weights\n",
        "   • Add TF-IDF features from text\n",
        "   • Try CLIP for better multimodal fusion\n",
        "   • Post-process outliers (clip extreme values)\n",
        "   • Add more domain-specific features\n",
        "\n",
        "📊 Top 5 Most Important Features:\n",
        "{importance_df.head(5).to_string(index=False)}\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"✅ READY FOR SUBMISSION!\")\n",
        "print(f\"📤 Upload: {output_filename}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deployment_class"
      },
      "source": [
        "## SECTION 21: Quick Inference Function (for future use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ensemble_predictor"
      },
      "outputs": [],
      "source": [
        "class EnsemblePredictor:\n",
        "    \"\"\"Complete predictor for deployment\"\"\"\n",
        "    \n",
        "    def __init__(self, dl_model, xgb_model, lgb_model, tokenizer, \n",
        "                 feature_cols, weights, device):\n",
        "        self.dl_model = dl_model\n",
        "        self.xgb_model = xgb_model\n",
        "        self.lgb_model = lgb_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_cols = feature_cols\n",
        "        self.weights = weights\n",
        "        self.device = device\n",
        "        \n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    \n",
        "    def predict_single(self, sample_id, catalog_content, image_link, image_dir):\n",
        "        \"\"\"Predict price for a single product\"\"\"\n",
        "        \n",
        "        # Feature engineering\n",
        "        df_single = pd.DataFrame([{\n",
        "            'sample_id': sample_id,\n",
        "            'catalog_content': catalog_content,\n",
        "            'image_link': image_link\n",
        "        }])\n",
        "        df_single = comprehensive_feature_engineering(df_single)\n",
        "        \n",
        "        # Deep Learning prediction\n",
        "        with torch.no_grad():\n",
        "            text = str(catalog_content)[:512]\n",
        "            encoding = self.tokenizer(text, max_length=128, padding='max_length',\n",
        "                                     truncation=True, return_tensors='pt')\n",
        "            \n",
        "            img_path = f'{image_dir}/{sample_id}.jpg'\n",
        "            try:\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "            except:\n",
        "                image = Image.new('RGB', (224, 224), color='white')\n",
        "            \n",
        "            image = self.transform(image).unsqueeze(0)\n",
        "            features = torch.tensor([float(df_single[col].iloc[0]) \n",
        "                                    for col in self.feature_cols]).unsqueeze(0)\n",
        "            \n",
        "            input_ids = encoding['input_ids'].to(self.device)\n",
        "            attention_mask = encoding['attention_mask'].to(self.device)\n",
        "            image = image.to(self.device)\n",
        "            features = features.to(self.device)\n",
        "            \n",
        "            dl_log_pred = self.dl_model(input_ids, attention_mask, image, features)\n",
        "            dl_pred = np.expm1(dl_log_pred.cpu().item())\n",
        "        \n",
        "        # XGBoost prediction\n",
        "        X_single = df_single[self.feature_cols].values\n",
        "        xgb_log_pred = self.xgb_model.predict(xgb.DMatrix(X_single))[0]\n",
        "        xgb_pred = np.expm1(xgb_log_pred)\n",
        "        \n",
        "        # LightGBM prediction\n",
        "        lgb_log_pred = self.lgb_model.predict(X_single)[0]\n",
        "        lgb_pred = np.expm1(lgb_log_pred)\n",
        "        \n",
        "        # Ensemble\n",
        "        final_pred = (dl_pred * self.weights['dl'] + \n",
        "                     xgb_pred * self.weights['xgb'] +\n",
        "                     lgb_pred * self.weights['lgb'])\n",
        "        \n",
        "        return round(max(0.01, final_pred), 2)\n",
        "\n",
        "print(\"\\n✓ EnsemblePredictor class ready for deployment!\")\n",
        "print(\"\\nYou can now use this notebook for production inference!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}